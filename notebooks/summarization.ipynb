{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8866f352-2f70-4f32-8bdb-40016a18e435",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11106a3-0434-470b-9cd8-314ea6891b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d344f97-c890-4032-ab8b-3011aba86054",
   "metadata": {},
   "source": [
    "### A) Basic Prompt\n",
    "\n",
    "Useful to summarize a few sentences or paragraphs. It's limited to the model context of around 4,096 tokens. It make only single call to the LLM and the LLM has access to all the data at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8e7209-9288-4c1d-bba8-47c2746c3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3ce0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Mojo is a new programming language for AI and is said to be 35000X faster than Python. \\\n",
    "With AI (Artificial Intelligence) on the rise, we need appropriate tools to build efficiently. \\\n",
    "Mojo was created by Chris Lattner, the creator of the Swift programming language and the LLVM Compiler Infrastructure. \\\n",
    "Lattner started working on Mojo in 2019, and the language was first released in May 2023. \\\n",
    "Mojo Lang is a programming language designed for AI hardware, such as GPUs with CUDA support. \\\n",
    "It accomplishes this through the use of Multi-Level Intermediate Representation (MLIR) \\\n",
    "to scale hardware varieties without complexity.\n",
    "'''\n",
    "message = [\n",
    "    SystemMessage(content='You are an expert copywriter with expertise in summarising documents'),\n",
    "    HumanMessage(content=f\"Please provide a short and concise summary of the following text:\\n TEXT: {text}\")\n",
    "]\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae4d7b8d-8176-43fe-b0e9-1ee8e7497077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa899a2-8544-4988-aae1-d89accdf7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_output = llm(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a490f4e7-9761-4982-a28a-c906d36fea02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mojo is a new programming language for AI that is 35000X faster than Python. Created by Chris Lattner, the creator of Swift and LLVM, Mojo is designed for AI hardware and uses Multi-Level Intermediate Representation (MLIR) to scale hardware varieties without complexity.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad29d2d-65a3-400b-b6b1-5074e475ed1f",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "This method is restricted to summarising text that together with the summary has a length lower than the model's maximum allowed number of allowed tokens i.e should not exit the token limit of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e30287e-f6ed-4ce5-82f7-f255e5f5afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c17a2f-e52e-4b48-9c05-856623117c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "Write a short and conscise summary of text: `{text}`\n",
    "Translate the summary to {language}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text', 'language'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b8b43c-2a41-47b4-8666-7d8967c45c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(prompt.format(text=text, language='English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b49c4d7-b8fe-4264-8091-2aff5a90ce6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mojo est un nouveau langage de programmation pour l'IA et est dit être 35000 fois plus rapide que Python. Avec l'IA en plein essor, nous avons besoin d'outils appropriés pour construire efficacement. Mojo a été créé par Chris Lattner, le créateur du langage de programmation Swift et de l'infrastructure du compilateur LLVM. Lattner a commencé à travailler sur Mojo en 2019 et le langage a été publié pour la première fois en mai 2023. Mojo Lang est un langage de programmation conçu pour le matériel d'IA, tel que les GPU avec prise en charge de CUDA. Il réalise cela grâce à l'utilisation de la Représentation Interne à Niveaux Multiples (MLIR) pour mettre à l'échelle les variétés de matériel sans complexité.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "summary = chain.run({'text': text, 'language': 'french'})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d468f5-bfe8-4b0f-9c1b-cea6b00515fd",
   "metadata": {},
   "source": [
    "### Summarizing using StuffDocumentsChain\n",
    "\n",
    "In Stuffing method, you stuff all the text to be summarized into the prompt as context to pass to the language model. This is similar to the methods shown previously, regardless of document size. It only makes a single call to the llm and when generating text, the llm has access to all the data at once. This won't work in large documents as this will result in the prompt being larger than the context length. It only works with smaller pieces of data and is not feasible with large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3067ea48-0404-4d70-b4cd-e92ae03ae89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3979c8-0375-4fa0-900f-a99eb1b33a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../files/sj.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "docs = [Document(page_content=text)]\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e8c331-12d2-4ac6-be01-e22e45ea2b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The speaker, Steve Jobs, shares three stories from his life during a commencement speech. The first story is about dropping out of college and how it led him to take a calligraphy class that later influenced the design of the Macintosh computer. The second story is about getting fired from Apple, which ultimately led him to start new ventures and find success. The third story is about facing death and how it reminded him of the importance of living life to the fullest. Jobs encourages the graduates to follow their passions, trust their intuition, and stay hungry and foolish in their pursuits.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = '''\n",
    "Write a short and conscise summary of text: `{text}`\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type='stuff', prompt=prompt, verbose=False)\n",
    "output = chain.run(docs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd7a76-cf2b-430c-b7b2-397451d1bded",
   "metadata": {},
   "source": [
    "### Summarizing Large Documents using MapReduce\n",
    "Used for large documents that exceed the token limit of the model.\n",
    "It splits the document into small chunks that fit in the token limit of the model. It then summarises each chunk and then gets a summary of the summary.\n",
    "Uses an initial prompt to summarise each chunk of data and another one to combine each summary into the final one.\n",
    "It scales to larger documents and the cost to the llm on individual chunks is independent and can run in parallel. It requires many more costs (API calls) to the llm than stuffDocument. Also, it loses some info during the final combining call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06d6b48b-77c8-4ed9-b38e-bd3a4cc3f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff235c4-c290-4431-a3f9-d466c5353641",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../files/sj.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09d856e9-b769-4708-8f11-9026cfce78e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2653"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5454236f-4bf5-40fe-a5be-38759ad0f31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents([text])\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bffad4cd-f7e8-4b69-958e-d62cb594fae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Steve Jobs shares three stories from his life, including dropping out of college and how it influenced the design of the Macintosh computer, getting fired from Apple and finding success in new ventures, and his experience with cancer and the importance of living each day fully. He encourages the audience to follow their passions, not settle, and remember that life is short. The speaker also reflects on the inevitability of death, urges the audience to live their own lives, and mentions The Whole Earth Catalog's message of staying hungry and foolish.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type='map_reduce', verbose=False)\n",
    "output_summary = chain.run(chunks)\n",
    "output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a205145c-3680-4ce7-bfaa-decd6a7a3404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt for summarizing each chunk\n",
    "chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfee3798-cb48-49f3-afa7-949c33f4322b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt for combining the summaries\n",
    "chain.combine_document_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e72a2b-49ce-4a20-aef2-009e8cf520ba",
   "metadata": {},
   "source": [
    "### MapReduce with custom prompts\n",
    "\n",
    "The defualt prompts in previous mapreduce are static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd28844e-5573-4abd-ac02-1821eed50cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First prompt used to summarise each chunk is called map_prompt\n",
    "map_prompt = '''\n",
    "Write a short and concise summary of the following:\n",
    "Text: ``{text}\n",
    "CONCISE SUMMARY:\n",
    "'''\n",
    "map_prompt_template = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=map_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b901564-7ab2-439e-a9fc-458e5a699d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second prompt that summarises the summaries\n",
    "combine_prompt='''\n",
    "Write a concise summary of the following text that covers the key points.\n",
    "Add a title to the summary.\n",
    "Start your summary with an INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED \n",
    "by BULLET POINTS if possible AND end the summary with a CONCLUSION PHRASE.\n",
    "Text: `{text}`\n",
    "'''\n",
    "combine_prompt_template = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=combine_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3093b0d8-fa41-4637-9b86-acbfc7c0a1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Steve Jobs' Speech on Life Lessons and Following Your Passion\n",
      "\n",
      "Introduction:\n",
      "In this speech, Steve Jobs shares three stories from his life, highlighting the importance of following one's passion and living life to the fullest. He discusses how dropping out of college, getting fired from Apple, and his experience with cancer shaped his perspective on life.\n",
      "\n",
      "Key Points:\n",
      "- Story 1: Jobs dropped out of college and took a calligraphy class, which later influenced the design of the Macintosh computer.\n",
      "- Story 2: Getting fired from Apple led Jobs to start new ventures and ultimately find success.\n",
      "- Story 3: Jobs' experience with cancer reminded him of the importance of living each day to the fullest.\n",
      "\n",
      "- Jobs emphasizes the significance of following one's passion and not settling for less.\n",
      "- He encourages readers to live their own lives and not be influenced by others' opinions.\n",
      "- The author shares a personal anecdote about The Whole Earth Catalog and the message of \"Stay Hungry. Stay Foolish.\"\n",
      "\n",
      "Conclusion:\n",
      "In conclusion, Steve Jobs' speech emphasizes the importance of following one's passion, not settling, and remembering that life is short. He encourages readers to live their own lives and make the most of each day.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    map_prompt=map_prompt_template,\n",
    "    combine_prompt=combine_prompt_template,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=False\n",
    ")\n",
    "output = chain.run(chunks)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b14ca-6f4c-4d7f-a560-45f78f6aee8c",
   "metadata": {},
   "source": [
    "### Using CombineDocumentChain\n",
    "\n",
    "Uses a refined chain. The document is split as well\n",
    "It summarises the first chunk of data, and the summary of the first chunk is passed into the second chunk.\n",
    "The LLM then refines the summary and passes it to the next chunk.\n",
    "This is done until the nth chuck\n",
    "- summarise(chunk #1) =< summary 1\n",
    "- summarise(summary #1 + chunk #2) =< summary 2\n",
    "- summarise(summary #2 + chunk #3) =< summary 3\n",
    "- ...\n",
    "- summarise(summary #n-1 + chunk #n) =< final summary\n",
    "\n",
    "It is less lossy than MapReduce and uses a more relevant context, resulting in better summarisation. However, it requires more calls to the LLM. These calls are not independent and cannot be parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4b185e3-96dc-4e68-a4c5-ddd96a8bad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65f4aa29-f4c4-4749-8b03-dac59cb4990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install unstructured -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0112be3f-dbfe-4815-ac8e-5c42812a53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pdf2image -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1f01c18-ffc5-4098-ad43-3088770e4733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loader = UnstructuredPDFLoader('../files/attention_is_all_you_need.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb71f0bc-8b3b-41aa-9126-4032b9626b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c897533e-72d7-4568-a2b4-8c15bcff2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07b5e15c-6cfc-4b20-8b92-1baf6d2d7461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05346100-9b11-4f07-a457-eb9c206fc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afec580b-082a-402f-9398-69d51696dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 9861\n",
      "Embedding Cost in USD: 0.019722\n"
     ]
    }
   ],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens/1000 * 0.002:.6f}')\n",
    "\n",
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "892a9368-407f-4508-882b-5baebc6bab03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper introduces the Transformer, a new network architecture based solely on attention mechanisms. The Transformer model achieves superior results in machine translation tasks and offers advantages such as parallelizability and reduced training time compared to existing models. The paper discusses the benefits of self-attention and describes the architecture of the Transformer model, which consists of stacked self-attention and fully connected layers in both the encoder and decoder. The attention mechanism used in the model is called Scaled Dot-Product Attention. Additionally, the paper explores the use of multi-head attention and positional encodings in the Transformer model. The authors compare self-attention layers to recurrent and convolutional layers in terms of computational complexity, parallelizability, and the ability to learn long-range dependencies. The paper also presents the training regime for the models, including the training data, hardware, optimizer, and regularization techniques used. The results show that the Transformer model outperforms previous state-of-the-art models in machine translation tasks and generalizes well to other tasks such as English constituency parsing. The authors express excitement about the future of attention-based models and plan to apply them to other tasks, including those involving input and output modalities other than text. They also plan to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio, and video. The code used to train and evaluate the models is available at the provided GitHub link. The authors acknowledge the contributions of Nal Kalchbrenner and Stephan Gouws. The paper also highlights that some attention heads in the Transformer model exhibit behavior related to the sentence structure, indicating that they have learned to perform different tasks.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type='refine', verbose=False)\n",
    "output_summary = chain.run(chunks)\n",
    "output_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f597cd8-0a50-4e74-be18-3633f9fb2958",
   "metadata": {},
   "source": [
    "### Summariizing using Refine chain and custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38a792ed-fbb9-4733-a50f-b1fb5bdb0788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title: The Transformer: A New Network Architecture Based on Attention Mechanisms\\n\\nIntroduction:\\nThe Transformer is a novel network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional neural networks. It has shown remarkable performance in machine translation tasks, offering improved quality, parallelizability, and reduced training time. The Transformer's versatility extends to other tasks, enabling it to compute input and output representations without relying on sequence-aligned RNNs or convolution.\\n\\nSummary:\\n\\n- The attention mechanism used in the Transformer is called Scaled Dot-Product Attention, which computes the dot products of queries and keys and applies a softmax function to obtain weights.\\n- The Transformer model incorporates multi-head attention, allowing it to jointly attend to information from different representation subspaces at different positions.\\n- The model architecture consists of stacked self-attention and fully connected layers for both the encoder and decoder.\\n- The Transformer employs positional encodings to inject information about the relative or absolute position of tokens in the sequence, enabling it to utilize the order of the sequence.\\n- In addition to attention sub-layers, each layer in the encoder and decoder contains a fully connected feed-forward network.\\n- The computational complexity of self-attention layers is lower than that of recurrent layers when the sequence length is smaller than the representation dimensionality.\\n- The maximum path length between any two input and output positions is shorter in self-attention layers compared to recurrent or convolutional layers.\\n- The Transformer model demonstrates significant improvements in computational efficiency and performance compared to existing models.\\n- Self-attention can yield more interpretable models, with attention heads exhibiting behavior related to the syntactic and semantic structure of sentences.\\n- Many attention heads in the Transformer exhibit behavior related to the structure of the sentence, indicating that they have learned to perform different tasks.\\n\\nConclusion:\\nThe Transformer network architecture, based on attention mechanisms, offers a promising alternative to traditional recurrent and convolutional neural networks. Its ability to achieve superior performance in machine translation tasks, along with its computational efficiency and parallelizability, make it a valuable tool in natural language processing and other sequence transduction tasks. The Transformer's generalizability is demonstrated by its success in English constituency parsing, outperforming previous models in this task as well. The future of attention-based models looks promising, with potential applications in various domains beyond text.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template= '''\n",
    "Write a short and concise summary of the following extracting the key information:\n",
    "Text: ``{text}\n",
    "CONCISE SUMMARY:\n",
    "'''\n",
    "\n",
    "initial_prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "refine_template = '''\n",
    "Produce a final summary.\n",
    "I have provided an existing summary up to a certain point: {existing_answer}. \n",
    "Please refine the existing summary with some more context below\n",
    "*******\n",
    "{text}\n",
    "*******\n",
    "Add a title to the summary.\n",
    "Start your summary with an INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED \n",
    "by BULLET POINTS if possible AND end the summary with a CONCLUSION PHRASE.\n",
    "'''\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=['existing_answer', 'text'],\n",
    "    template=refine_template\n",
    ")\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='refine',\n",
    "    question_prompt=initial_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=False\n",
    ")\n",
    "output_summary = chain.run(chunks)\n",
    "output_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef2a42-bb65-4467-9484-1f130ace2752",
   "metadata": {},
   "source": [
    "### Summarizing using LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a482c0d-66ab-4cf5-afb2-a9fd944bf6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4f36ad1-848f-4138-b136-019fe0c66ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "96d568ea-d96d-434f-99ee-51dd737162fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
    "wikipedia = WikipediaAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8cf239d9-1a10-4c24-90cb-d9d671307b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name='wikipedia',\n",
    "        func=wikipedia.run,\n",
    "        description=\"Useful for getting information about a topic from Wikipedia\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "621e9b37-2f4b-4db1-9e9e-26529fcef5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=False)\n",
    "output = agent_executor.run(\"Give a summary about King Jaja of Opobo Kingdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3227d2a4-dbdc-4aa1-a708-5f4f5bad503f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'King Jaja of Opobo Kingdom was a prominent leader in the Opobo Kingdom, which is located in Rivers State, Nigeria. The Opobo Kingdom is divided into 14 sections and King Jaja was one of the leaders of the Jaja section.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6192749-d16d-4e31-ab04-32d627073245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
